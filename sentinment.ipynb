from multiprocessing import reduction
import pandas as pd
import time
import numpy as np
import csv
import argparse
import math
from tqdm import tqdm
from sklearn.metrics import accuracy_score
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification, AutoModel, BertPreTrainedModel
from torch import cuda
import os
import torch.nn as nn
import torch.nn.functional as F
from transformers.modeling_outputs import TokenClassifierOutput
import warnings
from sklearn.model_selection import train_test_split

print(torch.version.cuda)
MAX_LEN = 512 # suitable for all datasets
BATCH_SIZE = 8
LEARNING_RATE = 1e-5
num_labels = 2
!ls
data = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv', delimiter=',', nrows = nRowsRead)
data[:5]
class dataset(Dataset):
    def __init__(self, sentences, labels, tokenizer):
        self.sentences = sentences
        self.labels = labels
        self.max_len = 512   #Max token input to bert
        self.tokenizer = tokenizer
        self.len = len(labels)

    def __getitem__(self, index):
        # step 1: tokenize sentence and adapt labels
        sentence = self.sentences[index]
        label = self.labels[index]
        # print(sentence)

        tokenized_sentence = ['[CLS]'] + sentence + ['[SEP]']

        # step 3: truncating or padding
        max_len = self.max_len
        #print(tokenized_sentence)
        if len(tokenized_sentence) > max_len:
            #truncate
            tokenized_sentence = tokenized_sentence[:max_len]
        else:
            # pad
            tokenized_sentence = tokenized_sentence + ['[PAD]' for _ in range(max_len - len(tokenized_sentence))]

        # step 4: obtain attention mask
        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]
        # step 5: convert tokens to input ids
        token_ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)
        target = []
        label_to_id = {'positive' : 0, 'negative' : 1}
        id_to_label = {0: 'positive', 1: 'negative'}
        label_id = label_to_id[label]
        target.append(label_id)

        return {
            'index': index,
            'ids': torch.tensor(token_ids, dtype=torch.long),
            'mask': torch.tensor(attn_mask, dtype=torch.long),
            'target': torch.tensor(target, dtype=torch.long)
        }

    def __len__(self):
        return self.len
        sentence_list = data['review'].tolist()
labels = data['sentiment'].tolist()
labels[:5]
sentence_list[:2]
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
tokenized_sentences = []
for sentence in sentence_list:
  split_sentence = list(sentence.split(' '))
  new_sentence = []
  for word in split_sentence:
    new_words = tokenizer.tokenize(word)
    new_sentence.extend(new_words)
  tokenized_sentences.append(new_sentence)
  full_dataset  = dataset(sentences=tokenized_sentences, labels = labels, tokenizer=tokenizer)
  full_dataset[0]
  train_size = int(0.8 * len(full_dataset))
devel_size = int(0.1 * len(full_dataset))
test_size = len(full_dataset) - train_size - devel_size
train_dataset, devel_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, devel_size, test_size])
devel_dataset[0]
class MainModel(BertPreTrainedModel):
    def __init__(self, config):
        super(MainModel,self).__init__(config)
        self.num_labels = num_labels
        self.bert = AutoModel.from_pretrained("bert-base-uncased")
        # two fully connected layers
        self.hidden_layer = nn.Linear(768, 2*(self.num_labels))
        self.classifier = nn.Linear(2*(self.num_labels),self.num_labels)

    def forward(self, input_ids, attention_mask, labels,device):

        output = self.bert(input_ids, attention_mask = attention_mask)
        output = output.last_hidden_state
        output = output[:,0,:]
        hidden_output = self.hidden_layer(output)
        classifier_out = self.classifier(hidden_output)
        main_prob = F.softmax(classifier_out, dim = 1)
        loss_main = F.cross_entropy(main_prob, labels.view(-1))
        return loss_main,main_prob
        def train(model, dataloader, optimizer, device):
    tr_loss, tr_accuracy = 0, 0
    bias_loss = 0
    # tr_preds, tr_labels = [], []
    nb_tr_steps = 0
    #put model in training mode
    model.train()

    for idx, batch in enumerate(dataloader):
        indexes = batch['index']
        input_ids = batch['ids'].to(device, dtype=torch.long)
        mask = batch['mask'].to(device, dtype=torch.long)
        targets = batch['target'].to(device, dtype=torch.long)

        loss_main, main_prob = model(input_ids=input_ids, attention_mask=mask, labels=targets,device = device)

        tr_loss += loss_main.item()
        nb_tr_steps += 1
        predicted_labels = torch.argmax(main_prob, dim=1)
        targets = targets.view(-1)
        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predicted_labels.cpu().numpy())
        tr_accuracy += tmp_tr_accuracy
        if idx % 100 == 0:
            print(f'\tModel loss at {idx} steps: {tr_loss}')
            if idx != 0:
                print(f'\tModel Accuracy : {tr_accuracy/nb_tr_steps}')
            with open('live.txt', 'a') as fh:
                fh.write(f'\tModel Loss at {idx} steps : {tr_loss}\n')
                if idx != 0:
                    fh.write(f'\tModel Accuracy : {tr_accuracy/nb_tr_steps}')
        torch.nn.utils.clip_grad_norm_(
            parameters = model.parameters(),
            max_norm = 10
        )
        optimizer.zero_grad()
        loss_main.backward()
        optimizer.step()


    print(f'\tModel loss for the epoch: {tr_loss}')
    print(f'\tTraining accuracy for epoch: {tr_accuracy/nb_tr_steps}')

def valid(model, dataloader, device):
    eval_loss = 0
    bias_loss = 0
    eval_accuracy = 0
    model.eval()
    nb_eval_steps = 0
    for batch in dataloader:
        indexes = batch['index']
        input_ids = batch['ids'].to(device, dtype=torch.long)
        mask = batch['mask'].to(device, dtype=torch.long)
        targets = batch['target'].to(device, dtype=torch.long)

        loss_main, main_prob = model(input_ids=input_ids, attention_mask=mask, labels=targets,device = device)
        eval_loss += loss_main.item()
        nb_eval_steps += 1
        #compute training accuracy
        predicted_labels = torch.argmax(main_prob, dim=1)
        # print(predicted_labels.shape)
        targets = targets.view(-1)
        # print(targets.shape)
        tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predicted_labels.cpu().numpy())
        eval_accuracy += tmp_eval_accuracy

    print(f'\tValidation accuracy for epoch: {eval_accuracy/nb_eval_steps}')

    return eval_loss, eval_accuracy/nb_eval_steps
    def inference(model, dataloader, tokenizer, device):
    model.eval()
    pred_lst = []
    test_loss = 0
    bias_loss = 0
    nb_test_steps = 0
    test_accuracy = 0
    for idx, batch in enumerate(tqdm(dataloader, ncols=100)):
        indexes = batch['index']
        input_ids = batch['ids'].to(device, dtype=torch.long)
        mask = batch['mask'].to(device, dtype=torch.long)
        targets = batch['target'].to(device, dtype=torch.long)
        with torch.no_grad():
            loss_main, main_prob = model(input_ids=input_ids, attention_mask=mask, labels=targets, device = device)
        test_loss += loss_main.item()
        nb_test_steps += 1
        predicted_labels = torch.argmax(main_prob, dim=1)
        # print(predicted_labels.shape)
        targets = targets.view(-1)
        # print(targets.shape)
        tmp_test_accuracy = accuracy_score(targets.cpu().numpy(), predicted_labels.cpu().numpy())
        test_accuracy += tmp_test_accuracy

    test_accuracy = test_accuracy / nb_test_steps
    return test_accuracy
    #main function

config = AutoConfig.from_pretrained("bert-base-uncased" , num_labels=num_labels)
model = MainModel.from_pretrained("bert-base-uncased", config = config)
optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)
device = 'cuda' if cuda.is_available() else 'cpu'
model.to(device)
train_dataloader = DataLoader(train_dataset, shuffle = True, batch_size=BATCH_SIZE)
devel_dataloader = DataLoader(devel_dataset, shuffle=True, batch_size=BATCH_SIZE)
test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=BATCH_SIZE)
num_epochs = 10
max_acc = 0.0
patience = 0
best_model = model
best_tokenizer = tokenizer
start = time.time()
for epoch in range(num_epochs):
    print(f'Epoch {epoch+1}:')
    train(model, train_dataloader, optimizer, device)
    validation_loss, eval_acc = valid(model, devel_dataloader, device)
    print(f'\tValidation loss: {validation_loss}')
    if eval_acc >= max_acc:
        max_acc = eval_acc
        patience = 0
        best_model = model
        best_tokenizer = tokenizer
    else:
        patience += 1
        if patience > 5:
            print("Early stopping at epoch : ",epoch)
            patience = 0
            break

end = time.time()
total_time = end - start

print(f"Total training time : {total_time}")

test_accuracy = inference(model, test_dataloader, tokenizer, device)
print(f'\t test accuracy: {test_accuracy}')
